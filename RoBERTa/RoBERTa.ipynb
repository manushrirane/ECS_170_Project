{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. SETUP AND IMPORTS\n",
        "# ==============================================================================\n",
        "# Install necessary libraries (Hugging Face Datasets, Transformers, PyTorch, scikit-learn)\n",
        "!pip install -q datasets transformers torch scikit-learn tqdm\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, roc_auc_score\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set seeds for reproducibility, matching project standard\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Determine the processing device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"Hardware device set to: {device}\")"
      ],
      "metadata": {
        "id": "KvFZpq-aKXV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 2. CONFIGURATION (HYPERPARAMETERS)\n",
        "# ==============================================================================\n",
        "MODEL_NAME = \"roberta-base\"\n",
        "DATA_DIR   = \"imdb_tokenized_256\"\n",
        "HEAD_PATH  = \"es_head_best.pt\" # Caching file for best model head weights\n",
        "\n",
        "# Evolution Strategies (ES) Hyperparameters\n",
        "POP = 64          # Population size (number of parameter candidates per iteration)\n",
        "SIGMA = 0.02      # Mutation noise scale (standard deviation of Gaussian noise)\n",
        "ITERS = 200       # Total number of ES iterations (generations)\n",
        "LR = 0.01         # Learning rate for the Adam optimizer used to update theta\n",
        "EVAL_STEPS_PER_CAND = 8 # Number of batches to evaluate each candidate on\n",
        "MAX_LENGTH = 256\n",
        "BATCH_TRAIN = 64\n",
        "BATCH_VAL = 128\n"
      ],
      "metadata": {
        "id": "5OAw4Yu2KnEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 3. DATA PREPARATION AND CACHING\n",
        "# ==============================================================================\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(\"STATUS: Data cache not found. Loading and tokenizing IMDb dataset.\")\n",
        "    # Load dataset directly from Hugging Face for simplicity in Colab\n",
        "    raw_dset = load_dataset(\"imdb\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH\n",
        "        )\n",
        "\n",
        "    # Tokenize and format the dataset\n",
        "    tokenized_dset = raw_dset.map(tokenize_function, batched=True)\n",
        "    tokenized_dset = tokenized_dset.rename_column(\"label\", \"labels\")\n",
        "    tokenized_dset = tokenized_dset.remove_columns([\"text\"])\n",
        "\n",
        "    # Create the 50/50 train/validation split\n",
        "    dset = DatasetDict({\n",
        "        \"train\": tokenized_dset[\"train\"].select(range(25000)),\n",
        "        \"validation\": tokenized_dset[\"train\"].select(range(25000, 50000)),\n",
        "    })\n",
        "\n",
        "    # Save to disk (disk-based caching)\n",
        "    dset.save_to_disk(DATA_DIR)\n",
        "    print(f\"STATUS: Dataset saved and cached to disk at '{DATA_DIR}'.\")\n",
        "else:\n",
        "    print(f\"STATUS: Loading cached dataset from '{DATA_DIR}'.\")\n",
        "    dset = DatasetDict.load_from_disk(DATA_DIR)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "# Prepare DataLoaders\n",
        "collate = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "train_loader = DataLoader(dset[\"train\"], batch_size=BATCH_TRAIN, shuffle=True, collate_fn=collate, num_workers=2)\n",
        "val_loader = DataLoader(dset[\"validation\"], batch_size=BATCH_VAL, shuffle=False, collate_fn=collate, num_workers=2)\n"
      ],
      "metadata": {
        "id": "9bYo2lmwKqs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 4. MODEL SETUP AND ES UTILITIES\n",
        "# ==============================================================================\n",
        "# Load the base model and freeze the main body (RoBERTa encoder)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "model.eval()\n",
        "\n",
        "# Flatten the classification head parameters (ES-trainable parameters)\n",
        "head_params = model.classifier.parameters()\n",
        "theta = torch.cat([p.flatten() for p in head_params]).to(device)\n",
        "DIM = theta.numel()\n",
        "\n",
        "@torch.no_grad()\n",
        "def unpack_head(theta_vec):\n",
        "    \"\"\"Restores the flattened theta vector into the model's classification head.\"\"\"\n",
        "    pointer = 0\n",
        "    for p in model.classifier.parameters():\n",
        "        num_elements = p.numel()\n",
        "        p.data.copy_(theta_vec[pointer:pointer + num_elements].view_as(p))\n",
        "        pointer += num_elements\n",
        "    return model\n",
        "\n",
        "def batch_loss(batch):\n",
        "    \"\"\"Calculates the loss for a single batch.\"\"\"\n",
        "    out = model(\n",
        "        input_ids=batch[\"input_ids\"].to(device),\n",
        "        attention_mask=batch[\"attention_mask\"].to(device),\n",
        "        labels=batch[\"labels\"].to(device),\n",
        "    )\n",
        "    return out.loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def val_accuracy():\n",
        "    \"\"\"Calculates full validation set accuracy for ES fitness.\"\"\"\n",
        "    correct, total = 0, 0\n",
        "    for batch in val_loader:\n",
        "        out = model(\n",
        "            input_ids=batch[\"input_ids\"].to(device),\n",
        "            attention_mask=batch[\"attention_mask\"].to(device),\n",
        "        )\n",
        "        preds = out.logits.argmax(-1).cpu()\n",
        "        y = batch[\"labels\"]\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.shape[0]\n",
        "    return correct / total\n",
        "\n",
        "# --- NEW METRIC: TRAINABLE PARAMETER FRACTION ---\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_fraction = trainable_params / total_params\n",
        "\n",
        "print(f\"\\nModel Configuration:\")\n",
        "print(f\"  Trainable Parameter Count: {trainable_params:,}\")\n",
        "print(f\"  Total Parameter Count:     {total_params:,}\")\n",
        "print(f\"  Trainable Fraction:        {trainable_fraction:.6f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NAJR12f0KuR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 5. EVOLUTION STRATEGIES TRAINING LOOP\n",
        "# ==============================================================================\n",
        "print(f\"\\nSTATUS: Starting ES Optimization ({ITERS} Iterations)...\")\n",
        "\n",
        "best_theta = theta.clone()\n",
        "best_acc = 0.0\n",
        "optimizer = torch.optim.Adam([theta], lr=LR)\n",
        "theta.requires_grad_(True)\n",
        "\n",
        "start_time = time.time()\n",
        "train_iterator = iter(train_loader)\n",
        "\n",
        "for t in tqdm(range(1, ITERS + 1), desc=\"ES Generations\"):\n",
        "    noises = torch.randn(POP // 2, DIM).to(device)\n",
        "    rewards = []\n",
        "\n",
        "    for eps in noises:\n",
        "        # Evaluate for +epsilon and -epsilon (Symmetric Sampling)\n",
        "        for sign in (+1.0, -1.0):\n",
        "            cand = theta.data + sign * SIGMA * eps\n",
        "            unpack_head(cand)\n",
        "\n",
        "            loss_sum, n = 0.0, 0\n",
        "            for _ in range(EVAL_STEPS_PER_CAND):\n",
        "                try:\n",
        "                    batch = next(train_iterator)\n",
        "                except StopIteration:\n",
        "                    train_iterator = iter(train_loader)\n",
        "                    batch = next(train_iterator)\n",
        "                loss_sum += batch_loss(batch).item()\n",
        "                n += 1\n",
        "            avg_loss = loss_sum / n\n",
        "            rewards.append(-avg_loss) # Reward is the negative of the loss\n",
        "\n",
        "    rewards = torch.tensor(rewards).to(device)\n",
        "\n",
        "    # Compute the ES gradient estimate\n",
        "    rewards_matrix = rewards.view(POP // 2, 2)\n",
        "    diff = (rewards_matrix[:, 0] - rewards_matrix[:, 1]).to(device) # r+ - r-\n",
        "    grad_estimate = torch.matmul(diff, noises) / (2 * POP * SIGMA)\n",
        "\n",
        "    # Update theta using Adam\n",
        "    theta.grad = -grad_estimate # Minimize -Reward = Loss\n",
        "    optimizer.step()\n",
        "\n",
        "    # Periodically check validation accuracy and save the best weights\n",
        "    if t % 10 == 0 or t == ITERS:\n",
        "        unpack_head(theta.data)\n",
        "        current_acc = val_accuracy()\n",
        "        print(f\"Iteration {t}/{ITERS} | Val Acc: {current_acc:.4f} | Best Acc: {best_acc:.4f}\")\n",
        "\n",
        "        if current_acc > best_acc:\n",
        "            best_acc = current_acc\n",
        "            best_theta.copy_(theta.data)\n",
        "            # Model Weights Caching\n",
        "            torch.save(unpack_head(best_theta).classifier.state_dict(), HEAD_PATH)\n",
        "            print(f\"INFO: New best head saved to: {HEAD_PATH}\")\n",
        "\n",
        "elapsed_train_time = time.time() - start_time\n",
        "print(f\"STATUS: ES Training complete. Duration: {elapsed_train_time:.1f} seconds.\")\n"
      ],
      "metadata": {
        "id": "x7B-O3cnK06x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 6. FINAL EVALUATION AND REPORTING\n",
        "# ==============================================================================\n",
        "print(\"\\nSTATUS: Starting Final Evaluation on Full Validation Set...\")\n",
        "\n",
        "# Load the best cached weights for final evaluation\n",
        "if os.path.exists(HEAD_PATH):\n",
        "    model.classifier.load_state_dict(torch.load(HEAD_PATH, map_location=device))\n",
        "    print(f\"INFO: Loaded best ES head from: {HEAD_PATH}\")\n",
        "else:\n",
        "    print(f\"WARNING: Best head weights file '{HEAD_PATH}' not found. Evaluating the final, un-cached ES head.\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Inference loop: Collect labels and LOGITS (for ROC AUC)\n",
        "labels, logits = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        out = model(\n",
        "            input_ids=batch[\"input_ids\"].to(device),\n",
        "            attention_mask=batch[\"attention_mask\"].to(device),\n",
        "        )\n",
        "        logits.append(out.logits.cpu().numpy())\n",
        "        labels.append(batch[\"labels\"].numpy())\n",
        "\n",
        "labels = np.concatenate(labels)\n",
        "logits = np.concatenate(logits)\n",
        "preds = logits.argmax(-1) # Hard predictions\n",
        "\n",
        "# --- NEW METRIC: ROC AUC ---\n",
        "# Calculate the probability of the positive class (class 1)\n",
        "probs = torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\n",
        "roc_auc = roc_auc_score(labels, probs)\n",
        "\n",
        "# Standard Metrics\n",
        "acc = float(accuracy_score(labels, preds))\n",
        "f1  = float(f1_score(labels, preds, average=\"macro\"))\n",
        "report = classification_report(labels, preds, digits=4)\n",
        "cm = confusion_matrix(labels, preds)\n",
        "\n",
        "# Print Final Summary Report\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"FINAL PERFORMANCE SUMMARY: {MODEL_NAME} + Evolution Strategies (Head-Only)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Metric: Val Accuracy:                {acc:.4f}\")\n",
        "print(f\"Metric: Macro-F1:                    {f1:.4f}\")\n",
        "print(f\"Metric: ROC AUC:                     {roc_auc:.4f}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Efficiency: ES Optimization Time (s):  {elapsed_train_time:.1f}\")\n",
        "print(f\"Efficiency: Trainable Param Fraction: {trainable_fraction:.6f}\")\n",
        "print(\"\\nClassification Report:\\n\", report)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Save Outputs (Report and Predictions Caching)\n",
        "pd.DataFrame({\"label\": labels, \"pred\": preds}).to_csv(\"es_val_predictions.csv\", index=False)\n",
        "with open(\"es_report.txt\", \"w\") as f:\n",
        "    f.write(f\"Val Accuracy: {acc:.6f}\\n\")\n",
        "    f.write(f\"Macro-F1: {f1:.6f}\\n\")\n",
        "    f.write(f\"ROC AUC: {roc_auc:.6f}\\n\")\n",
        "    f.write(f\"Trainable Parameters: {trainable_params:,}\\n\")\n",
        "    f.write(f\"Trainable Fraction: {trainable_fraction:.6f}\\n\")\n",
        "    f.write(f\"ElapsedTrainSec: {elapsed_train_time:.1f}\\n\\n\")\n",
        "    f.write(\"Classification report:\\n\")\n",
        "    f.write(report + \"\\n\")\n",
        "    f.write(\"Confusion matrix:\\n\")\n",
        "    f.write(str(cm))\n",
        "\n",
        "print(\"\\nSTATUS: Final report saved to 'es_report.txt'\")\n",
        "print(\"STATUS: Validation predictions saved to 'es_val_predictions.csv'\")"
      ],
      "metadata": {
        "id": "mIf8JcRsLATC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}